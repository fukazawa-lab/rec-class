# -*- coding: utf-8 -*-
"""LLM.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z0xw36QwD6kCAxLzfnJ3DXR2Cv-evNoW

# 大規模言語モデルのファインチューニング

# 1 環境の準備
"""
import argparse
import torch
from transformers.trainer_utils import set_seed
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import numpy as np
import pandas as pd
from transformers import Trainer
from transformers import AutoModelForSequenceClassification
from sklearn.metrics import accuracy_score, precision_score, recall_score
from pprint import pprint
from datasets import Dataset, ClassLabel
from datasets import load_dataset
import pandas as pd
from typing import Union
from transformers import BatchEncoding
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import r2_score

def main(epoch_num, model_name):
    # 乱数シードを42に固定
    set_seed(42)
    print("乱数シード設定完了")

    """# 2 データセットの準備"""

    from sklearn.preprocessing import MinMaxScaler

    # CSVファイルからデータを読み込む
    original_train_df = pd.read_csv('/content/rec-class/dataset/training_bert.csv')
    valid_df = pd.read_csv('/content/rec-class/dataset/validation_bert.csv')

    # ラベルの正規化用にMinMaxScalerを作成
    scaler = MinMaxScaler()

    # trainデータのラベルを正規化
    original_train_df['label'] = scaler.fit_transform(original_train_df[['label']])

    # validデータのラベルを正規化
    valid_df['label'] = scaler.transform(valid_df[['label']])

    # 正規化後のデータをDatasetに変換
    train_dataset = Dataset.from_pandas(original_train_df)
    valid_dataset = Dataset.from_pandas(valid_df)

    # pprintで見やすく表示する
    pprint(train_dataset[0])
    print("")

    """# 3. トークン化"""

    # モデル名を指定してトークナイザを読み込む
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # トークン化のクラス名を確認
    print(type(tokenizer).__name__)

    # テキストのトークン化
    tokens = tokenizer.tokenize(train_dataset[0]['sentence'])
    print(tokens)

    # データのトークン化
    def preprocess_text_classification(example: dict[str, str | int]) -> BatchEncoding:
        """文書分類の事例のテキストをトークナイズし、IDに変換"""
        encoded_example = tokenizer(example["sentence"], max_length=512)
        input_tokens = tokenizer.convert_ids_to_tokens(encoded_example["input_ids"])
        encoded_example["labels"] = float(example["label"])  # ラベルをFloat型に変換
        return encoded_example

    encoded_train_dataset = train_dataset.map(
        preprocess_text_classification,
        remove_columns=train_dataset.column_names,
    )
    encoded_valid_dataset = valid_dataset.map(
        preprocess_text_classification,
        remove_columns=valid_dataset.column_names,
    )

    # トークン化の確認
    print(encoded_train_dataset[0])

    """# 4 ミニバッチ構築"""

    from transformers import DataCollatorWithPadding
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    batch_inputs = data_collator(encoded_train_dataset[0:4])
    pprint({name: tensor.size() for name, tensor in batch_inputs.items()})

    """# 5 モデルの準備"""

    from transformers import AutoModelForSequenceClassification
    from collections import Counter

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    num_labels = 1

    model = (AutoModelForSequenceClassification
        .from_pretrained(model_name, num_labels=1)
        .to(device))

    model.resize_token_embeddings(len(tokenizer))

    """# 6 訓練の実行"""

    from transformers import TrainingArguments

    training_args = TrainingArguments(
        output_dir="output_wrime",  # 結果の保存フォルダ
        per_device_train_batch_size=8,  # 訓練時のバッチサイズ
        per_device_eval_batch_size=8,  # 評価時のバッチサイズ
        learning_rate=2e-5,  # 学習率
        lr_scheduler_type="linear",  # 学習率スケジューラの種類
        warmup_ratio=0.1,  # 学習率のウォームアップの長さを指定
        num_train_epochs=epoch_num,  # エポック数
        save_strategy="epoch",  # チェックポイントの保存タイミング
        logging_strategy="epoch",  # ロギングのタイミング
        evaluation_strategy="epoch",  # 検証セットによる評価のタイミング
        load_best_model_at_end=True,  # 訓練後に開発セットで最良のモデルをロード
        metric_for_best_model="1/mse",  # 最良のモデルを決定する評価指標
        fp16=False,  # 修正: FP16を無効にする
        fp16_full_eval=False,  # 修正: FP16 full evalを無効にする
    )

    def compute_metrics_for_regression(eval_pred):
        logits, labels = eval_pred
        labels = labels.reshape(-1, 1)
        
        mse = mean_squared_error(labels, logits)
        mae = mean_absolute_error(labels, logits)
        r2 = r2_score(labels, logits)
        single_squared_errors = ((logits - labels).flatten()**2).tolist()
        
        # Compute accuracy 
        accuracy = 1 / mse
        
        return {"mse": mse, "mae": mae, "r2": r2, "1/mse": accuracy}

    trainer = Trainer(
        model=model,
        train_dataset=encoded_train_dataset,
        eval_dataset=encoded_valid_dataset,
        data_collator=data_collator,
        args=training_args,
        compute_metrics=compute_metrics_for_regression,
    )
    trainer.train()

    latest_eval_metrics = trainer.evaluate()
    print(latest_eval_metrics)

    predictions = trainer.predict(encoded_valid_dataset)

    # 通常は0番目のラベルに対応する予測値
    predictions_df = pd.DataFrame({
        'userId_movieId': valid_df["userId_movieId"],
        'label': valid_dataset["label"],
        'sentence': valid_dataset["sentence"],
        'predicted_value': predictions.predictions.flatten()
    })

    # MinMaxScalerで元のスケールに戻す
    original_label = scaler.inverse_transform(predictions_df[['label']])
    original_predicted_labels = scaler.inverse_transform(predictions_df[['predicted_value']])

    # 通常は0番目のラベルに対応する予測値
    predictions_df_2 = pd.DataFrame({
        'userId_movieId': valid_df["userId_movieId"],
        # 'label': original_label.flatten(),
        # 'sentence': valid_dataset["sentence"],
        'rating': original_predicted_labels.flatten()
    })

    # 予測結果をCSVに保存
    predictions_df_2.to_csv("/content/rec-class/dataset/validation_predictions_llm.csv", index=False)

    mse_original_scale = mean_squared_error(original_label, original_predicted_labels)
    mae_original_scale = mean_absolute_error(original_label, original_predicted_labels)
    rmse_original_scale = np.sqrt(mse_original_scale)

    # Display the results
    print("MSE:", mse_original_scale)
    print("MAE:", mae_original_scale)
    print("RMSE:", rmse_original_scale)

    """# 7 テストデータの予測"""
    test_file_path = '/content/rec-class/dataset/test_bert.csv'
    if not os.path.exists(test_file_path):
        print("test_bert.csv が見つからないため、テストデータの予測をスキップします。")
        return

    # テストデータの読み込み
    test_df = pd.read_csv(test_file_path)

    # テストデータの読み込み
    test_df = pd.read_csv('/content/rec-class/dataset/test_bert.csv')

    # テストデータのトークン化（ラベルは仮に0が入っているため無視）
    def preprocess_test_text_classification(example: dict[str, str | int]) -> BatchEncoding:
        encoded_example = tokenizer(example["sentence"], max_length=512)
        return encoded_example

    test_dataset = Dataset.from_pandas(test_df)
    encoded_test_dataset = test_dataset.map(
        preprocess_test_text_classification,
        remove_columns=test_dataset.column_names,
    )

    # 予測
    test_predictions = trainer.predict(encoded_test_dataset)

    # MinMaxScalerを使って元のスケールに戻す
    original_test_predictions = scaler.inverse_transform(test_predictions.predictions)

    # テストデータの予測値を保存
    test_predictions_df = pd.DataFrame({
        'userId_movieId': test_df["userId_movieId"],
        'rating': original_test_predictions.flatten()
    })

    # 予測結果をCSVに保存
    test_predictions_df.to_csv("/content/rec-class/dataset/submission_llm.csv", index=False)

    print("提出用ファイル作成完了しました。submission_llm.csvをダウンロードしてKaggleに登録ください.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Fine-tune a language model.")
    parser.add_argument("--epoch_num", type=int, default=10, help="Number of epochs for training.")
    parser.add_argument("--model_name", type=str, default="bert-base-uncased", help="Model name for training.")

    args = parser.parse_args()

    main(args.epoch_num, args.model_name)
